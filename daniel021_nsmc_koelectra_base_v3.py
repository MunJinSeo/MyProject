# -*- coding: utf-8 -*-
"""daniel021_nsmc_koelectra_base_v3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19J_SBAAgPdHOYrXypXeEIvAdeKbcbUOs

# 한국어(NSMC) KoELECTRA를_이용한_감정분석기: Colab(Pytorch + HuggingFace)
# Github : https://github.com/MunJinSeo/MyProject/
<br>

## References 1
- (1) 김희규님의 "HuggingFace KoElectra로 NSMC 감성분석 Fine-tuning해보기"<br>
https://heegyukim.medium.com/huggingface-koelectra%EB%A1%9C-nsmc-%EA%B0%90%EC%84%B1%EB%B6%84%EB%A5%98%EB%AA%A8%EB%8D%B8%ED%95%99%EC%8A%B5%ED%95%98%EA%B8%B0-1a23a0c704af

- (2) 이지원님의 Github : nlp_emotion_classification <br>
https://github.com/jiwonny/nlp_emotion_classification

## 사용모델 KoELECTRA
- 한국어 : 박장원님의 KoELECTRA 사용<br>
https://monologg.kr/2020/05/02/koelectra-part1/<br>
https://github.com/monologg/KoELECTRA

## Dataset (학습에는 train만 사용함, test는 감점대상)
- 한국어 : 네이버 영화 리뷰 데이터셋<br>
https://github.com/e9t/nsmc <br>
ratings_train.txt <br>
ratings_test.txt

## 과제 파일
- https://www.kaggle.com/c/korean-sa-competition-bdc101 <br>
ko_data.csv

## References 2
- https://colab.research.google.com/drive/1tIf0Ugdqg4qT7gcxia3tL7und64Rv1dP
- https://blog.naver.com/horajjan/221739630055
<br>@전처리 관련@<br>
- https://github.com/YongWookHa/kor-text-preprocess
- https://github.com/likejazz/korean-sentence-splitter
- https://github.com/lovit/soynlp
<br>@@<br>
- https://huggingface.co/transformers/training.html
- https://tutorials.pytorch.kr/beginner/data_loading_tutorial.html
- https://tutorials.pytorch.kr/beginner/blitz/cifar10_tutorial.html
- https://wikidocs.net/44249


## 실행 방법 / 유의사항
- Colab 에서 반드시 GPU로 실행 (Colab Pro 권장)
- 과제파일(ko_data.csv)은 사전에 직접 호스팅 서버로 업로드 해줘야함
- 모델 학습을 위한 NSMC 데이터셋은 자동 다운로드 처리됨 (train만 사용함)
- 소스는 위에서부터 순차적으로 실행하면 됨
- CUDA(GPU) 메모리 오버되는경우 학습시 Batch size 줄여서 해볼것
- KoELECTRA base_v3으로 사용했으며 1epoch 당 약 60분

## 처리 순서 
- 필요 lib 설치
- NSMC 데이터셋 다운로드
- 필요 모듈 import
- 데이터셋 처리 (Dataset Calss / 전처리)
- 모델 생성 (Create Model)
- 학습(Learn) - train파일만 사용(test제외)
- 테스트 데이터셋 정확도 확인하기
- 모델 저장하기
- 과제용 데이터 예측 및 맵핑
- 결과 파일 저장

##--------------------------------------------------------------------

# 필요 lib 설치
"""

# lib 설치
!pip install transformers
!pip install torch

!pip install kss
!pip install konlpy
!pip install sentencepiece
!pip install soynlp

# (미사용) Colab TPU 사용을 위해 설치
#--!pip install torch_xla
#--#@param ["1.5" , "20200325", "nightly"]
#VERSION = "1.7"
#!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py
#!python pytorch-xla-env-setup.py --version $VERSION

"""# NSMC 데이터셋 다운로드"""

#NSMC 데이터셋 다운로드

#!git clone https://github.com/e9t/nsmc.git

# 학습에는 train만 사용함, test는 감점대상
#!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt
!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt
!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt

# ko_data.csv #별도 kaggle에서 받아서 사용 https://www.kaggle.com/c/korean-sa-competition-bdc101

#!head ratings.txt
!head ratings_train.txt
!head ratings_test.txt
# !head ko_data.csv #별도 kaggle에서 받아서 사용 https://www.kaggle.com/c/korean-sa-competition-bdc101

"""# 필요 모듈 import"""

import pandas as pd
import torch
from torch.nn import functional as F
from torch.utils.data import DataLoader, Dataset
from transformers import AutoTokenizer, ElectraTokenizer, ElectraForSequenceClassification, AdamW
from tqdm.notebook import tqdm

# (미사용) TPU 사용을 위해 필요
#import torch_xla
#import torch_xla.core.xla_model as xm

import kss
import re
from soynlp.normalizer import *

# GPU or CPU
if torch.cuda.is_available():
  device = torch.device("cuda")
  print('There are %d GPU(s) available.' % torch.cuda.device_count())
  print('We will use the GPU:', torch.cuda.get_device_name(0))
else:
  device = torch.device("cpu")
  print('No GPU available, using the CPU instead.')

# (미사용) TPU
#device = xm.xla_device()

"""# 데이터셋 처리 (Dataset Calss / 전처리)
(train data와 과제 sample data형식이 다르고, encoding이 다르기 때문에 분리 처리)
"""

class NSMC_Dataset(Dataset):
  
  def __init__(self, csv_file, ftype):
    # train data와 sample data 각각 처리

    # 초기 전처리 1, 아래쪽 전처리 2 로 clean_text() 펑션 분리함 (학습 할때 line별 전처리 후 사용됨)
    # --- 전처리 1 start ------------------------

    if ftype == 'train':
      # 일부 값중에 NaN 제거
      self.dataset = pd.read_csv(csv_file, sep='\t').dropna(axis=0)

    elif ftype == 'sample':
      # 한글처리용 cp949적용
      self.dataset = pd.read_csv(csv_file, sep=',', encoding='cp949') 
       # 뒤쪽 컬럼 label 값이 없으므로 기본값으로 추가함
      self.dataset.insert(2,'Predicted','-')
      #print(self.dataset)
    else:
      self.dataset = pd.read_csv(csv_file, sep=',').dropna(axis=0)


    # for idx, document in self.dataset.iterrows():
    #   print(idx, document)
    self.sub1 = re.compile('[^ .?!/@$%~|0-9|ㄱ-ㅣ가-힣]+') # 한글과 띄어쓰기, 특수기호 일부를 제외한 모든 글자제거
    self.sub2 = re.compile('[\s]+')  # white space duplicate
    self.sub3 = re.compile('[\.]+')  # full stop duplicate

    #중복되는 문장 제거
    if ftype == 'sample':
      print('과제 제출용은 중복 문장 제거하면 안됨')
    else:
      # 중복제거: document가 리뷰 텍스트 내용의 title명이다
      self.dataset.drop_duplicates(subset=['document'], inplace=True)

    # dataset 확인
    print(self.dataset.describe())
    print(self.dataset)

    # tokenizer
    #self.tokenizer = AutoTokenizer.from_pretrained("monologg/koelectra-small-v3-discriminator")
    self.tokenizer = ElectraTokenizer.from_pretrained("monologg/koelectra-base-v3-discriminator")
    
    # --- 전처리 1 end ------------------------
  
  def __len__(self):
    return len(self.dataset)
  
  def clean_text(self, txt):
    # --- 전처리 2 start ----------------------
    cleaned = self.sub1.sub('', txt.strip())  # .strip()은 문장의 앞뒤 공백제거함
    cleaned = self.sub2.sub(' ', cleaned)
    cleaned = self.sub3.sub('.', cleaned)
    cleaned = emoticon_normalize(cleaned, num_repeats=3) # 감정 반복 단순화
    cleaned = repeat_normalize(cleaned, num_repeats=2) # 중복 글자 단순화
    #cleaned = only_text(cleaned) # text만 추출
    #cleaned = only_hangle(cleaned) # 한글만 추출
    #cleaned = only_hangle_number(cleaned) # 한글/숫자만 추출

    # 문장 분리하여 일정 길이 넘는 것만 사용
    ttStr = ""
    for ssStr in kss.split_sentences(cleaned):
      #print(ssStr)
      if len(ssStr) > 1:
        #ttStr += ssStr + " \n"
        ttStr += "[CLS] " + ssStr + " [SEP]"

    cleaned = ttStr
    # --- 전처리 2 end ------------------------
    return cleaned

  
  def __getitem__(self, idx):
    # 행번호별 컬럼 지정하여 할당
    row = self.dataset.iloc[idx, 1:3].values  # idx번째 행의 첫번째 컬럼 0을 제외하고 1~3컬럼 할당
    text = self.clean_text( txt=row[0] ) # 전처리 2 : clean_text()
    y = row[1]

    inputs = self.tokenizer(
        text, 
        return_tensors='pt',
        truncation=True,
        max_length=256,
        pad_to_max_length=True,
        add_special_tokens=True
        )
    
    input_ids = inputs['input_ids'][0]
    attention_mask = inputs['attention_mask'][0]

    return input_ids, attention_mask, y

#학습에는 train만 사용함, test는 감점대상
train_dataset = NSMC_Dataset("ratings_train.txt","train")
test_dataset = NSMC_Dataset("ratings_test.txt","train")
sample_dataset = NSMC_Dataset("ko_data.csv","sample")

tmpstr = '훌륭하다. 초한지 얼른 읽어보고 다시 봐야겠다. 연출 훌륭하다 껄껄 한신의 토사구팽은 슬펐다'
print( train_dataset.clean_text( txt = tmpstr) )

"""# 모델 생성 (Create Model)"""

model = ElectraForSequenceClassification.from_pretrained("monologg/koelectra-base-v3-discriminator").to(device)
model.cuda()

# 한번 실행해보기
#text, attention_mask, y = train_dataset[0]
#model(text.unsqueeze(0).to(device), attention_mask=attention_mask.unsqueeze(0).to(device))

try:
  model.load_state_dict(torch.load("model.pt"))
except:
  print("error - model.load_state_dict(torch.load('model.pt'))")
else:
  print("success - model.load_state_dict(torch.load('model.pt'))")

# 모델 레이어 보기
model

"""# 학습(Learn) - train파일만 사용(test제외)"""

# koelectra-small-v3-discriminator
#epochs = 16
#batch_size = 128

# koelectra-base-v3-discriminator
epochs = 12
batch_size = 32 #CUDA(GPU) 사용량 오버되면 Batch size를 줄여서 해볼것

optimizer = AdamW(model.parameters(), 
                  lr=1e-5, # 학습률
                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값
                  )
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

losses = []
accuracies = []

# 그래디언트 초기화
# model.zero_grad()

for i in range(epochs):
  total_loss = 0.0
  correct = 0
  total = 0
  batches = 0

  # 훈련모드로 변경
  model.train()

  for input_ids_batch, attention_masks_batch, y_batch in tqdm(train_loader):
    optimizer.zero_grad()
    y_batch = y_batch.to(device)
    y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]
    loss = F.cross_entropy(y_pred, y_batch)
    loss.backward() # Backward 수행으로 그래디언트 계산
    #xm.optimizer_step(optimizer, barrier=True)  # TPU 사용시 코드
    optimizer.step() # 그래디언트를 통해 가중치 파라미터 업데이트
    #model.zero_grad() # 그래디언트 초기화

    total_loss += loss.item()

    _, predicted = torch.max(y_pred, 1)
    correct += (predicted == y_batch).sum()
    total += len(y_batch)

    batches += 1
    if batches % 100 == 0:
      print("Batch Loss:", total_loss, "Accuracy:", correct.float() / total)
  
  losses.append(total_loss)
  accuracies.append(correct.float() / total)
  print("Train Loss:", total_loss, "Accuracy:", correct.float() / total)

losses, accuracies

"""# 테스트 데이터셋 정확도 확인하기"""

# 평가모드로 변경
model.eval()

test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)

test_correct = 0
test_total = 0

for input_ids_batch, attention_masks_batch, y_batch in tqdm(test_loader):
  y_batch = y_batch.to(device)
  y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]
  _, predicted = torch.max(y_pred, 1)
  test_correct += (predicted == y_batch).sum()
  test_total += len(y_batch)

print("Accuracy:", test_correct.float() / test_total)

"""# 모델 저장하기"""

# 모델 저장하기
torch.save(model.state_dict(), "model.pt")

"""# 과제용 데이터 예측 및 맵핑"""

#과제용 데이터 예측
# 데이터 로딩
batchSize = 16
sample_loader = DataLoader(sample_dataset, batch_size=batchSize, shuffle=False)

sample_result = sample_dataset.dataset.copy(deep=True)
print(sample_result)

#평가모드로 변경
model.eval()

idx_s = 0
idx_e = 0

for input_ids_batch, attention_masks_batch, y_batch in tqdm(sample_loader):
  #y_batch = y_batch.to(device)
  y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]
  _, predicted = torch.max(y_pred, 1)

  rsList = list(map(int, predicted)) # 결과를 한번에 저장하기 위해 LIST로 변환 처리
  global idx_s, idx_e
  idx_e += len(rsList) #해당 배치구간내에 index 끝값 계산
  #print("index==", idx_s, idx_e)
  #print("--start-- sample_result['predict_label'][idx_s : idx_e]==\n" , sample_result['predict_label'][idx_s : idx_e] )
  sample_result['Predicted'][idx_s : idx_e] = rsList  #배치구간을 한번에 업데이트
  #print("--end-- sample_result['predict_label'][idx_s : idx_e]==\n" , sample_result['predict_label'][idx_s : idx_e] )
  idx_s += len(rsList) #해당 배치구간내에 index 시작값은 윗줄 처리 후 증가

  #test_correct += (predicted == y_batch).sum()
  #test_total += len(y_batch)

# print("Accuracy:", test_correct.float() / test_total)
print(sample_result)

#torch.cuda.empty_cache() #GPU 캐쉬 데이터 삭제

"""# 결과 파일 저장"""

# 주어진 데이터의 결과를 파일로 저장
#sample_csv = sample_result.to_csv('sample_ko.csv')
sample_csv = sample_result.to_csv('sample_ko.csv',sep=',',na_rep='NaN', columns=['Id','Predicted'],index=False)

# 파일을 PC로 다운로드 하기
from google.colab import files
files.download('sample_ko.csv')

# 나의 구글Drive 연결
#from google.colab import drive
#drive.mount('/content/drive')