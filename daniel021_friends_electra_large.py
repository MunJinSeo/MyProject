# -*- coding: utf-8 -*-
"""daniel021_friends_electra_large.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QlCbpJNcuvljwxve953lKdNPxknafI_2

# 영어(Friends) ELECTRA를_이용한_감정분석기 : Colab(Pytorch + HuggingFace)
# Github : https://github.com/MunJinSeo/MyProject/
<br>

## References 1
- (1) 김희규님의 "HuggingFace KoElectra로 NSMC 감성분석 Fine-tuning해보기"<br>
https://heegyukim.medium.com/huggingface-koelectra%EB%A1%9C-nsmc-%EA%B0%90%EC%84%B1%EB%B6%84%EB%A5%98%EB%AA%A8%EB%8D%B8%ED%95%99%EC%8A%B5%ED%95%98%EA%B8%B0-1a23a0c704af

- (2) 이지원님의 Github : nlp_emotion_classification <br>
https://github.com/jiwonny/nlp_emotion_classification

## 사용모델 ELECTRA
- 영어 : 구글 ELECTRA 사용<br>
https://github.com/google-research/electra <br>
https://huggingface.co/google/electra-large-discriminator<br>
https://huggingface.co/google/electra-base-discriminator<br>
https://huggingface.co/google/electra-small-discriminator<br>

## Dataset (학습에는 train만 사용함, test는 감점대상)
- 영어 : Freinds <br>
http://doraemon.iis.sinica.edu.tw/emotionlines/ <br>
friends_train.json <br>
friends_test.json <br>
friends_dev.json

## 과제 파일- 영어
- https://www.kaggle.com/c/english-sa-competition-bdc101 <br>
en_data.csv

## References 2
- https://colab.research.google.com/drive/1tIf0Ugdqg4qT7gcxia3tL7und64Rv1dP
- https://blog.naver.com/horajjan/221739630055
<br>@전처리 관련@<br>
- https://github.com/YongWookHa/kor-text-preprocess
- https://github.com/likejazz/korean-sentence-splitter
- https://github.com/lovit/soynlp
<br>@@<br>
- https://huggingface.co/transformers/training.html
- https://tutorials.pytorch.kr/beginner/data_loading_tutorial.html
- https://tutorials.pytorch.kr/beginner/blitz/cifar10_tutorial.html
- https://wikidocs.net/44249


## 실행 방법 / 유의사항
- Colab 에서 반드시 GPU로 실행 (Colab Pro 권장)
- 과제파일(en_data.csv)은 사전에 서버로 업로드
- 모델 학습을 위한 Friends 데이터셋은 사전에 서버로 업로드<br>
 학습파일:friends_train.json <br>
 검증파일 :friends_test.json (friends_dev.json)
- 소스는 위에서부터 순차적으로 실행하면 됨
- CUDA(GPU) 메모리 오버되는경우 학습시 Batch size 줄여서 해볼것, 단, 너무 줄이는경우 성능 떨어짐
- 만약 정확도 확인하는 부문에서 CUDA(GPU) 메모리 오버가 나오면 ▷ 모델 별도 저장 ▷ 런타임 초기화 ▷ 모델 로딩 ▷ 학습은 건너뛰고 정확도 실행
- ELECTRA large 사용했으며 1epoch 당 약 30분

## 처리 순서 
- 필요 lib 설치
- Friends 데이터셋 처리
- 필요 모듈 import
- 데이터셋 처리 (Dataset Calss / 전처리)
- 모델 생성 (Create Model)
- 학습(Learn) - train파일만 사용(test제외)
- 테스트 데이터셋 정확도 확인하기
- 모델 저장하기
- 과제용 데이터 예측 및 맵핑
- 결과 파일 저장

##--------------------------------------------------------------------

# 필요 lib 설치
"""

# lib 설치
!pip install transformers
!pip install torch

#!pip install kss
#!pip install konlpy
#!pip install sentencepiece
#!pip install soynlp

# (미사용) Colab TPU 사용을 위해 설치
#--!pip install torch_xla
#--#@param ["1.5" , "20200325", "nightly"]
#VERSION = "1.7"
#!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py
#!python pytorch-xla-env-setup.py --version $VERSION

"""# Friends 데이터셋 처리"""

#Friends 데이터셋 처리
#zip파일로 업로드시 아래 사용, Colab에 직접 올려서 사용함
#!unzip friends_json.zip

# 학습에는 train만 사용함, test는 감점대상
!head friends_train.json
!head friends_test.json
!head friends_dev.json
# !head en_data.csv #별도 kaggle에서 받아서 사용 https://www.kaggle.com/c/english-sa-competition-bdc101

"""# 필요 모듈 import"""

import pandas as pd
import torch
from torch.nn import functional as F
from torch.utils.data import DataLoader, Dataset
from transformers import AutoTokenizer, ElectraTokenizer, ElectraForSequenceClassification, AdamW
from tqdm.notebook import tqdm

# (미사용) TPU 사용을 위해 필요
#import torch_xla
#import torch_xla.core.xla_model as xm

import json
import numpy as np

#import kss
import re
#from soynlp.normalizer import *

# GPU or CPU
if torch.cuda.is_available():
  device = torch.device("cuda")
  print('There are %d GPU(s) available.' % torch.cuda.device_count())
  print('We will use the GPU:', torch.cuda.get_device_name(0))
else:
  device = torch.device("cpu")
  print('No GPU available, using the CPU instead.')

# (미사용) TPU
#device = xm.xla_device()

"""# 데이터셋 처리 (Dataset Class / 전처리)
(train data와 과제 sample data형식이 다르고, encoding이 다르기 때문에 분리 처리)
"""

class Friends_Dataset(Dataset):
  # Json파일을 DataFrame으로 변환 함수
  def jsonToDataFrame(self, file_name):
    with open(file_name, encoding = 'utf-8', mode = 'r') as file:
      json_array = json.load(file)

    result = pd.DataFrame.from_dict(json_array[0])
   
    is_first = True
    for array in json_array:
      if is_first:
        is_first = False
        continue
      
      temp_df = pd.DataFrame.from_dict(array)
      result = result.append(temp_df, ignore_index = True)

    return result

  def __init__(self, csv_file, ftype):
    # train data와 sample data 각각 처리

    # 초기 전처리 1, 아래쪽 전처리 2 로 clean_text() 펑션 분리함 (학습 할때 line별 전처리 후 사용됨)
    # --- 전처리 1 start ------------------------
    self.emotion_dic = {'neutral':0,'surprise':1,'fear':2,'non-neutral':3,'joy':4,'sadness':5,'anger':6,'disgust':7}

    if ftype == 'train':
      #1 speaker, 2 utterance(표현문장), 3 emotion(감정분류값), 4 annotation
      self.dataset = self.jsonToDataFrame(file_name=csv_file) #.dropna(axis=0)

    elif ftype == 'sample':
      #1 id, 2 i_dialog, 3 i_utterance, 4 speaker, 5 utterance
      #self.dataset = pd.read_csv(csv_file, sep=',', encoding = 'unicode_escape')
      self.dataset = pd.read_csv(csv_file, sep=',', encoding = 'utf-8')
      # 뒤쪽 컬럼 label 값이 없으므로 기본값으로 추가함
      #0 idx, 1 id, 2 i_dialog, 3 i_utterance, 4 speaker, 5 utterance(표현문장),6 emotion, 7 Predicted 
      self.dataset.insert(5,'emotion',0) # 기본값 셋팅
      self.dataset.insert(6,'Predicted',0) # 기본값 셋팅
      #print(self.dataset)
    else:
      self.dataset = pd.read_csv(csv_file, sep=',')


    # for idx, document in self.dataset.iterrows():
    #   print(idx, document)
    self.sub1 = re.compile('[^ .?!/@$%~|0-9|a-zㅣA-Z]+') # 영어와 띄어쓰기, 특수기호 일부를 제외한 모든 글자제거
    self.sub2 = re.compile('[\s]+')  # white space duplicate
    self.sub3 = re.compile('[\.]+')  # full stop duplicate

    #중복되는 문장 제거
    if ftype == 'sample':
      print('과제 제출용은 중복 문장 제거하면 안됨')
    else:
      # 중복제거: utterance가 텍스트 내용의 title명이다
      self.dataset.drop_duplicates(subset=['utterance'], inplace=True)

    # dataset 확인
    print(self.dataset.describe())
    print(self.dataset)

    # tokenizer
    self.tokenizer = ElectraTokenizer.from_pretrained("google/electra-large-discriminator")
    
    # --- 전처리 1 end ------------------------
  
  def __len__(self):
    return len(self.dataset)
  
  def clean_text(self, txt):
    # --- 전처리 2 start ----------------------
    cleaned = self.sub1.sub('', txt.strip())  # .strip()은 문장의 앞뒤 공백제거함
    cleaned = self.sub2.sub(' ', cleaned)
    cleaned = self.sub3.sub('.', cleaned)
    #cleaned = emoticon_normalize(cleaned, num_repeats=3) # 감정 반복 단순화
    #cleaned = repeat_normalize(cleaned, num_repeats=2) # 중복 글자 단순화
    #cleaned = only_text(cleaned) # text만 추출
    #cleaned = only_hangle(cleaned) # 한글만 추출
    #cleaned = only_hangle_number(cleaned) # 한글/숫자만 추출

    #if len(ssStr) > 1:
    cleaned = "[CLS] " + cleaned + " [SEP]"
    # --- 전처리 2 end ------------------------
    return cleaned

  
  def __getitem__(self, idx):
    # 행번호별 컬럼 지정하여 할당: 0 idx, 1 speaker, 2 utterance, 3 emotion, 4 annotation
    #row = self.dataset.iloc[idx, 1:4].values  # idx번째 행의 첫번째 컬럼 0을 제외하고 1~3컬럼
    #print(row[0], row[1])
    #text = self.clean_text( txt= str(row[0]) ) # 전처리 2 : clean_text()
    #y = row[1] #숫자만 가능함 

    text = self.dataset['utterance'][idx]
    tmp_y = self.dataset['emotion'][idx]
    # emotion 값을 숫자로 치환
    y = 0
    if tmp_y in self.emotion_dic.keys() :
      y = self.emotion_dic[tmp_y]
    else :
      y = 0
    #print("[%s][%d]" % (tmp_y,y) )

    inputs = self.tokenizer(
        text, 
        return_tensors='pt',
        truncation=True,
        max_length=256,
        pad_to_max_length=True,
        add_special_tokens=True
        )
    
    input_ids = inputs['input_ids'][0]
    attention_mask = inputs['attention_mask'][0]

    return input_ids, attention_mask, y

# 학습에는 train만 사용함, test는 감점대상
train_dataset = Friends_Dataset("friends_train.json","train")
train_dataset.dataset = train_dataset.dataset.reset_index() #학습시 keyerror 발생시 사용

#test데이터는 감점사항으로 제외함
#train2_dataset = Friends_Dataset("friends_test.json","train")
#train_dataset.dataset = train_dataset.dataset.append(train2_dataset.dataset, ignore_index = True)
#print("train_dataset.dataset==ALL=\n", train_dataset.dataset)
#print("train_dataset.dataset.head()==\n", train_dataset.dataset.head())

test_dataset = Friends_Dataset("friends_test.json","train")
test_dataset.dataset = test_dataset.dataset.reset_index() #검증시 keyerror 발생시 사용

sample_dataset = Friends_Dataset("en_data.csv","sample")
#print("sample_dataset.dataset.head()==\n", sample_dataset.dataset.head())

# tmpstr = 'Come on.  Hello?  I?m sorry you have the wrong number.   Okay, I?ll call you later dad. I love you.'
# print( train_dataset.clean_text( txt = tmpstr) )

# test_emotion = 'joy'
# if test_emotion in train_dataset.emotion_dic.keys() :
#   print( train_dataset.emotion_dic[test_emotion] )
# else :
#   print(0)

# print( train_dataset.__getitem__(11790) )

# print( sample_dataset.__getitem__(10) )

"""# 모델 생성 (Create Model)"""

model = ElectraForSequenceClassification.from_pretrained("google/electra-large-discriminator", num_labels=8).to(device)
#model = ElectraForSequenceClassification.from_pretrained('google/electra-small-generator', num_labels=8)
#model.cuda()

# 한번 실행해보기
#text, attention_mask, y = train_dataset[0]
#model(text.unsqueeze(0).to(device), attention_mask=attention_mask.unsqueeze(0).to(device))

try:
  model.load_state_dict(torch.load("model.pt"))
  #model.load_state_dict(torch.load("/content/drive/MyDrive/Colab Notebooks/model_daniel021_friends_electra_large_epoch16.pt"))
except:
  print("error - model.load_state_dict(torch.load(...))")
else:
  print("success - model.load_state_dict(torch.load(...))")

# 모델 레이어 보기
model

"""# 학습(Learn) - train파일만 사용함(test제외)"""

# google/electra-small-discriminator
#epochs = 16
#batch_size = 64 #CUDA(GPU) 메모리 오버되는경우 size 줄여서 해볼것

# google/electra-base-discriminator
# google/electra-large-discriminator
epochs = 16
batch_size = 32 #CUDA(GPU) 메모리 오버되는경우 size 줄여서 해볼것, 단 너무 줄일경우 성능 떨어짐

optimizer = AdamW(model.parameters(), 
                  lr=1e-5, # 학습률
                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값
                  )
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

losses = []
accuracies = []

# 그래디언트 초기화
# model.zero_grad()

for i in range(epochs):
  total_loss = 0.0
  correct = 0
  total = 0
  batches = 0

  # 훈련모드로 변경
  model.train()

  for input_ids_batch, attention_masks_batch, y_batch in tqdm(train_loader):
    optimizer.zero_grad()
    y_batch = y_batch.to(device)
    y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]
    loss = F.cross_entropy(y_pred, y_batch)
    loss.backward() # Backward 수행으로 그래디언트 계산
    #xm.optimizer_step(optimizer, barrier=True)  # TPU 사용시 코드
    optimizer.step() # 그래디언트를 통해 가중치 파라미터 업데이트
    #model.zero_grad() # 그래디언트 초기화

    total_loss += loss.item()

    _, predicted = torch.max(y_pred, 1)
    #print("y_batch====\n",y_batch)
    #print("predicted====\n",predicted)
    correct += (predicted == y_batch).sum()
    total += len(y_batch)

    batches += 1
    if batches % 100 == 0:
      print("Batch Loss:", total_loss, "Accuracy:", correct.float() / total)
  
  losses.append(total_loss)
  accuracies.append(correct.float() / total)
  print("Train Loss:", total_loss, "Accuracy:", correct.float() / total)

losses, accuracies

"""# 테스트 데이터셋 정확도 확인하기"""

# 평가모드로 변경
model.eval()

test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)

test_correct = 0
test_total = 0

# 만약 정확도 확인하는 부문에서 CUDA(GPU) 메모리 오버가 나오면 ▷ 모델 별도 저장 ▷ 런타임 초기화 ▷ 모델 로딩 ▷ 학습은 건너뛰고 정확도 실행
for input_ids_batch, attention_masks_batch, y_batch in tqdm(test_loader):
  y_batch = y_batch.to(device)
  y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]
  _, predicted = torch.max(y_pred, 1)
  test_correct += (predicted == y_batch).sum()
  test_total += len(y_batch)

print("Accuracy:", test_correct.float() / test_total)

"""# 모델 저장하기"""

# 모델 저장하기
torch.save(model.state_dict(), "model.pt")

"""# 과제용 데이터 예측 및 맵핑"""

#과제용 데이터 예측
# 데이터 로딩
batchSize = 8
sample_loader = DataLoader(sample_dataset, batch_size=batchSize, shuffle=False)

sample_result = sample_dataset.dataset.copy(deep=True)
print(sample_result)

#평가모드로 변경
model.eval()

idx_s = 0
idx_e = 0

emotion_strDic = {0:'neutral',1:'surprise',2:'fear',3:'non-neutral',4:'joy',5:'sadness',6:'anger',7:'disgust'}

for input_ids_batch, attention_masks_batch, y_batch in tqdm(sample_loader):
  #y_batch = y_batch.to(device)
  y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]
  _, predicted = torch.max(y_pred, 1)

  rsList = list(map(int, predicted)) # 결과를 한번에 저장하기 위해 LIST로 변환 처리
  global idx_s, idx_e
  idx_e += len(rsList) #해당 배치구간내에 index 끝값 계산
  #print("index==", idx_s, idx_e)
  
  sample_result['emotion'][idx_s : idx_e] = rsList
  rsPred = [emotion_strDic[xx] for xx in rsList] # 숫자를 감정문자로 치환
  sample_result['Predicted'][idx_s : idx_e] = rsPred  #배치구간을 한번에 업데이트
  idx_s += len(rsList) #해당 배치구간내에 index 시작값은 윗줄 처리 후 증가

print(sample_result)

#torch.cuda.empty_cache() #GPU 캐쉬 데이터 삭제

"""# 결과 파일 저장"""

# 주어진 데이터의 결과를 파일로 저장
#sample_csv = sample_result.to_csv('sample.csv')
sample_csv = sample_result.to_csv('sample_en.csv',sep=',',na_rep='NaN', columns=['id','Predicted'],index=False)

# 파일을 PC로 다운로드 하기
from google.colab import files
files.download('sample_en.csv')

# 나의 구글Drive 연결하기
#from google.colab import drive
#drive.mount('/content/drive')